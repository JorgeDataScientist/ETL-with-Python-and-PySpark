{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa la clase SparkSession del módulo pyspark.sql.\n",
    "# SparkSession es la entrada principal para usar la API de Spark SQL.\n",
    "# Proporciona un entorno unificado para trabajar con estructuras de datos, ejecutar consultas SQL y más.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importa la clase SparkContext del módulo pyspark.\n",
    "# SparkContext es el núcleo de cualquier aplicación de Spark y actúa como una interfaz para acceder al cluster de Spark.\n",
    "# Es responsable de la comunicación entre la aplicación y los nodos ejecutores.\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración e Inicio de una Sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una sesión de Spark utilizando la clase SparkSession.\n",
    "# La sesión de Spark es la entrada principal para trabajar con DataFrames y la API de Spark SQL.\n",
    "spark  = SparkSession \\\n",
    "          .builder \\\n",
    "          .appName(\"Demo Spark\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación del Código: Crear una Sesión de Spark\n",
    "\n",
    "#### Detalle de cada línea:\n",
    "\n",
    "1. **`SparkSession`:**  \n",
    "   Representa el punto de entrada principal para trabajar con Apache Spark. Facilita el uso de DataFrames y la API de Spark SQL.\n",
    "\n",
    "2. **`.builder`:**  \n",
    "   Inicia la configuración para una nueva sesión de Spark. Es el lugar donde se definen las propiedades iniciales, como el nombre de la aplicación y otros ajustes opcionales.\n",
    "\n",
    "3. **`.appName(\"Demo_Spark\")`:**  \n",
    "   Especifica un nombre para la aplicación. Este nombre se usa para identificar la aplicación en las herramientas de administración y monitorización de Spark, como su interfaz web.\n",
    "\n",
    "4. **`.getOrCreate()`:**  \n",
    "   - Si ya existe una sesión de Spark activa en el contexto, la reutiliza.  \n",
    "   - Si no hay una sesión activa, crea una nueva con las configuraciones definidas previamente.\n",
    "\n",
    "### Notas adicionales:\n",
    "- Este código es esencial para inicializar el entorno de trabajo en Spark y comenzar a procesar datos.\n",
    "- Configuraciones adicionales, como la asignación de memoria, recursos o la URL del cluster, pueden incluirse en la fase de construcción (`builder`).\n",
    "- El nombre definido en `.appName()` es útil para identificar las aplicaciones en entornos con múltiples trabajos Spark en ejecución."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETL-with-Python-and-PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
