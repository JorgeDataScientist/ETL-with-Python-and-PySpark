{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa la clase SparkSession del módulo pyspark.sql.\n",
    "# SparkSession es la entrada principal para usar la API de Spark SQL.\n",
    "# Proporciona un entorno unificado para trabajar con estructuras de datos, ejecutar consultas SQL y más.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importa la clase SparkContext del módulo pyspark.\n",
    "# SparkContext es el núcleo de cualquier aplicación de Spark y actúa como una interfaz para acceder al cluster de Spark.\n",
    "# Es responsable de la comunicación entre la aplicación y los nodos ejecutores.\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración e Inicio de una Sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una sesión de Spark utilizando la clase SparkSession.\n",
    "# La sesión de Spark es la entrada principal para trabajar con DataFrames y la API de Spark SQL.\n",
    "Session_spark  = SparkSession \\\n",
    "          .builder \\\n",
    "          .appName(\"Demo_Spark\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación del Código: Crear una Sesión de Spark\n",
    "\n",
    "#### Detalle de cada línea:\n",
    "\n",
    "1. **`SparkSession`:**  \n",
    "   Representa el punto de entrada principal para trabajar con Apache Spark. Facilita el uso de DataFrames y la API de Spark SQL.\n",
    "\n",
    "2. **`.builder`:**  \n",
    "   Inicia la configuración para una nueva sesión de Spark. Es el lugar donde se definen las propiedades iniciales, como el nombre de la aplicación y otros ajustes opcionales.\n",
    "\n",
    "3. **`.appName(\"Demo_Spark\")`:**  \n",
    "   Especifica un nombre para la aplicación. Este nombre se usa para identificar la aplicación en las herramientas de administración y monitorización de Spark, como su interfaz web.\n",
    "\n",
    "4. **`.getOrCreate()`:**  \n",
    "   - Si ya existe una sesión de Spark activa en el contexto, la reutiliza.  \n",
    "   - Si no hay una sesión activa, crea una nueva con las configuraciones definidas previamente.\n",
    "\n",
    "### Notas adicionales:\n",
    "- Este código es esencial para inicializar el entorno de trabajo en Spark y comenzar a procesar datos.\n",
    "- Configuraciones adicionales, como la asignación de memoria, recursos o la URL del cluster, pueden incluirse en la fase de construcción (`builder`).\n",
    "- El nombre definido en `.appName()` es útil para identificar las aplicaciones en entornos con múltiples trabajos Spark en ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceder a SparkContext desde SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpContext = Session_spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detalle de la línea:\n",
    "\n",
    "1. **`SpContext = spark.sparkContext`:**  \n",
    "   - **`spark`:** Es la instancia de `SparkSession` que se ha creado previamente en el código.\n",
    "   - **`sparkContext`:** Es una propiedad de la clase `SparkSession` que proporciona acceso al objeto `SparkContext`. `SparkContext` es el componente central que se utiliza para interactuar con el cluster de Spark y distribuir las tareas entre los nodos.\n",
    "   - **`SpContext`:** Es la variable donde se almacena el objeto `SparkContext`, permitiendo interactuar con él para realizar operaciones en el cluster de Spark, como crear RDDs, configurar el entorno de ejecución, entre otras tareas.\n",
    "\n",
    "#### Notas adicionales:\n",
    "- Aunque `SparkContext` se puede acceder directamente a través de `SparkSession`, algunas tareas avanzadas requieren interactuar con `SparkContext` directamente. Por ejemplo, crear RDDs o trabajar con configuraciones de bajo nivel.\n",
    "- En la mayoría de los casos, el acceso a `sparkContext` se realiza a través de la instancia de `SparkSession`, como se hace aquí, para facilitar el manejo de recursos y la inicialización del entorno de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
      "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
      "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
      "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
      "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Session_spark.read.csv('cars.csv', header=True, sep=\";\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conociendo la Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Car: string (nullable = true)\n",
      " |-- MPG: string (nullable = true)\n",
      " |-- Cylinders: string (nullable = true)\n",
      " |-- Displacement: string (nullable = true)\n",
      " |-- Horsepower: string (nullable = true)\n",
      " |-- Weight: string (nullable = true)\n",
      " |-- Acceleration: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# El método printSchema() se utiliza para imprimir la estructura del esquema del DataFrame.\n",
    "# Esto incluye los nombres de las columnas, los tipos de datos asociados a cada columna y si estas pueden contener valores nulos (nullable).\n",
    "# Es especialmente útil para verificar la estructura de los datos antes de realizar transformaciones o análisis.\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación del Dataset y Plan para el ETL\n",
    "\n",
    "### Descripción del Esquema del Dataset\n",
    "\n",
    "El dataset contiene información de automóviles con las siguientes columnas:\n",
    "\n",
    "- **Car** (`string`, nullable): El nombre y modelo del automóvil.\n",
    "- **MPG** (`string`, nullable): Millas por galón que puede recorrer el automóvil (rendimiento).\n",
    "- **Cylinders** (`string`, nullable): Número de cilindros del motor.\n",
    "- **Displacement** (`string`, nullable): Cilindrada o volumen de desplazamiento del motor (generalmente en pulgadas cúbicas).\n",
    "- **Horsepower** (`string`, nullable): Potencia del motor en caballos de fuerza.\n",
    "- **Weight** (`string`, nullable): Peso del automóvil (posiblemente en libras).\n",
    "- **Acceleration** (`string`, nullable): Tiempo que tarda en acelerar de 0 a 60 mph (en segundos).\n",
    "- **Model** (`string`, nullable): Año del modelo del automóvil.\n",
    "- **Origin** (`string`, nullable): Procedencia del automóvil (e.g., `US`, `Europe`, `Japan`).\n",
    "\n",
    "#### Observaciones del Esquema\n",
    "1. Todas las columnas tienen tipo `string`, lo cual no es óptimo para realizar análisis o cálculos numéricos. Necesitamos convertirlas a los tipos de datos correctos (e.g., `double` o `int`).\n",
    "2. La columna `nullable` indica que puede haber valores nulos en estas columnas, por lo que debemos manejarlos durante el proceso de limpieza de datos.\n",
    "\n",
    "### Plan para el ETL (Extract, Transform, Load)\n",
    "\n",
    "#### 1. **Extracción**\n",
    "   - Leeremos los datos desde el archivo `cars.csv` utilizando PySpark, asegurándonos de que los encabezados sean detectados correctamente.\n",
    "\n",
    "#### 2. **Transformación**\n",
    "   - **Conversión de Tipos de Datos**: \n",
    "     Convertir las columnas numéricas (`MPG`, `Cylinders`, `Displacement`, `Horsepower`, `Weight`, `Acceleration`, `Model`) a sus tipos adecuados (`double` o `int`).\n",
    "   - **Manejo de Valores Nulos**: \n",
    "     Imputar valores nulos en las columnas donde sea necesario o eliminar filas con datos incompletos si no son significativos.\n",
    "   - **Normalización de Datos**:\n",
    "     Si fuera necesario, escalar las columnas numéricas para estandarizar los valores.\n",
    "   - **Creación de Nuevas Columnas** (Opcional): \n",
    "     Añadir columnas derivadas de las existentes, como categorías basadas en `MPG` (e.g., bajo, medio, alto consumo).\n",
    "\n",
    "#### 3. **Carga**\n",
    "   - Guardar el DataFrame limpio y transformado en un formato adecuado (e.g., CSV, Parquet) para su posterior análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2: Transformación\n",
    "\n",
    "#### 1. **Conversión de Tipos de Datos**\n",
    "Para realizar análisis y cálculos correctos, las columnas numéricas deben ser convertidas a sus tipos adecuados:\n",
    "\n",
    "- **Columnas a convertir:**\n",
    "  - `MPG`: Rendimiento, convertir a tipo `double`.\n",
    "  - `Cylinders`: Número entero de cilindros, convertir a tipo `int`.\n",
    "  - `Displacement`: Cilindrada del motor, convertir a tipo `double`.\n",
    "  - `Horsepower`: Potencia en caballos, convertir a tipo `double`.\n",
    "  - `Weight`: Peso del vehículo, convertir a tipo `double`.\n",
    "  - `Acceleration`: Tiempo de aceleración, convertir a tipo `double`.\n",
    "  - `Model`: Año del modelo, convertir a tipo `int`.\n",
    "\n",
    "**Acción**: Utilizaremos la función `cast()` de PySpark para cambiar los tipos de datos.\n",
    "\n",
    "#### 2. **Manejo de Valores Nulos**\n",
    "Los valores nulos pueden afectar el análisis. Realizaremos las siguientes acciones según la columna:\n",
    "- **Estrategias:**\n",
    "  - Para columnas numéricas: Sustituir los valores nulos por la media o mediana de la columna (imputación).\n",
    "  - Para columnas categóricas (e.g., `Origin`): Sustituir los valores nulos con el valor más frecuente (moda).\n",
    "  - Si hay demasiados valores nulos en ciertas filas, podrían eliminarse.\n",
    "\n",
    "**Acción**: Utilizaremos funciones como `fillna()` o `dropna()` para manejar estos casos.\n",
    "\n",
    "#### 3. **Normalización de Datos**\n",
    "Estandarizar los valores de las columnas numéricas para asegurar que estén en rangos similares, lo cual es importante para modelos de Machine Learning.\n",
    "- **Estrategias:**\n",
    "  - Aplicar normalización Min-Max o estandarización (z-score) en columnas como `Weight`, `Horsepower` y `Acceleration`.\n",
    "\n",
    "**Acción**: Utilizaremos funciones de PySpark como `StandardScaler` o realizaremos operaciones manuales con `withColumn`.\n",
    "\n",
    "#### 4. **Creación de Nuevas Columnas (Opcional)**\n",
    "Podemos derivar información adicional:\n",
    "- **Ejemplo**: Categorizar `MPG` en niveles de consumo:\n",
    "  - `MPG < 15`: Bajo consumo.\n",
    "  - `15 ≤ MPG ≤ 25`: Consumo medio.\n",
    "  - `MPG > 25`: Alto consumo.\n",
    "\n",
    "**Acción**: Utilizaremos `withColumn()` y expresiones condicionales (`when` y `otherwise`) de PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Conversión de Tipos de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Convertir los tipos de datos de las columnas utilizando cast().\n",
    "data = data \\\n",
    "    .withColumn(\"MPG\", data[\"MPG\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"Cylinders\", data[\"Cylinders\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"Displacement\", data[\"Displacement\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"Horsepower\", data[\"Horsepower\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"Weight\", data[\"Weight\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"Acceleration\", data[\"Acceleration\"].cast(DoubleType())) \\\n",
    "    .withColumn(\"Model\", data[\"Model\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Car: string (nullable = true)\n",
      " |-- MPG: double (nullable = true)\n",
      " |-- Cylinders: integer (nullable = true)\n",
      " |-- Displacement: double (nullable = true)\n",
      " |-- Horsepower: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Acceleration: double (nullable = true)\n",
      " |-- Model: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Manejo de Valores Nulos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Contar valores nulos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------------+----------+------+------------+-----+------+\n",
      "|Car|MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
      "+---+---+---------+------------+----------+------+------------+-----+------+\n",
      "|  0|  0|        0|           0|         0|     0|           0|    0|     0|\n",
      "+---+---+---------+------------+----------+------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Imputación de valores nulos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí calculamos la media de todas las columnas numéricas (excepto las de tipo texto como \"Car\" y \"Origin\") usando `mean()`. Luego usamos `fillna()` para rellenar los valores nulos con la media correspondiente de cada columna.\n",
    "\n",
    "mean_values = data.select([mean(c).alias(c) for c in data.columns if c != \"Car\" and c != \"Origin\"]).first()\n",
    "data = data.fillna(mean_values.asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Eliminar filas con valores nulos en columnas clave:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si consideramos que las columnas \"MPG\", \"Cylinders\", \"Displacement\", \"Horsepower\", \"Weight\", y \"Acceleration\" son esenciales para el análisis, podemos eliminar las filas que contienen valores nulos en cualquiera de estas columnas usando `dropna()`.\n",
    "\n",
    "data = data.dropna(subset=[\"MPG\", \"Cylinders\", \"Displacement\", \"Horsepower\", \"Weight\", \"Acceleration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideraciones:\n",
    "- **Imputación con la media** es adecuada cuando los datos no presentan una distribución sesgada y cuando no se cuenta con un valor específico para imputar.\n",
    "- **Eliminación de filas** puede ser apropiada si las filas con valores nulos son pocas o si los datos restantes son lo suficientemente representativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el punto de **Normalización de Datos**, existen dos estrategias comunes: **Min-Max Normalization** y **Standardization (z-score)**. A continuación, vamos a proceder con ambos enfoques utilizando PySpark.\n",
    "\n",
    "### 1. **Normalización Min-Max (Escalado entre 0 y 1)**\n",
    "La normalización Min-Max transforma los valores de las columnas numéricas a un rango [0, 1].\n",
    "\n",
    "### 2. **Estandarización (z-score)**\n",
    "La estandarización, también conocida como normalización Z-score, transforma los valores de las columnas para que tengan una media de 0 y una desviación estándar de 1.\n",
    "\n",
    "### Código para la Normalización y Estandarización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler, StandardScaler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vectors\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n",
      "File \u001b[1;32mf:\\Estudios de ProGramacion\\ETL-with-Python-and-PySpark\\Lib\\site-packages\\pyspark\\ml\\__init__.py:22\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one or more\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# contributor license agreements.  See the NOTICE file distributed with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mDataFrame-based machine learning APIs to let users quickly assemble and configure practical\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mmachine learning pipelines.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[0;32m     25\u001b[0m     Predictor,\n\u001b[0;32m     26\u001b[0m     PredictionModel,\n\u001b[0;32m     27\u001b[0m     Transformer,\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n",
      "File \u001b[1;32mf:\\Estudios de ProGramacion\\ETL-with-Python-and-PySpark\\Lib\\site-packages\\pyspark\\ml\\base.py:40\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     Any,\n\u001b[0;32m     25\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m since\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m P\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inherit_doc\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     HasInputCol,\n\u001b[0;32m     44\u001b[0m     HasOutputCol,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     Params,\n\u001b[0;32m     49\u001b[0m )\n",
      "File \u001b[1;32mf:\\Estudios de ProGramacion\\ETL-with-Python-and-PySpark\\Lib\\site-packages\\pyspark\\ml\\param\\__init__.py:32\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     Any,\n\u001b[0;32m     22\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JavaObject\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseVector, Vector, Matrix\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Para normalización Min-Max, primero necesitamos convertir las columnas a un formato vectorial.\n",
    "# Aplicamos la normalización Min-Max a las columnas `Weight`, `Horsepower`, y `Acceleration`\n",
    "\n",
    "# Convertir las columnas a formato vectorial para MinMaxScaler\n",
    "assembler = VectorAssembler(inputCols=[\"Weight\", \"Horsepower\", \"Acceleration\"], outputCol=\"features\")\n",
    "data_with_features = assembler.transform(data)\n",
    "\n",
    "# Crear el objeto MinMaxScaler y ajustarlo a los datos\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(data_with_features)\n",
    "data_normalized = scaler_model.transform(data_with_features)\n",
    "\n",
    "# Ahora, `data_normalized` contiene las columnas normalizadas en el campo `scaled_features`.\n",
    "data_normalized.select(\"Car\", \"scaled_features\").show(5)\n",
    "\n",
    "# Para estandarización (z-score), necesitamos transformar las mismas columnas\n",
    "scaler_standard = StandardScaler(inputCol=\"features\", outputCol=\"standardized_features\", withStd=True, withMean=True)\n",
    "scaler_standard_model = scaler_standard.fit(data_with_features)\n",
    "data_standardized = scaler_standard_model.transform(data_with_features)\n",
    "\n",
    "# Mostrar resultados de la estandarización (z-score)\n",
    "data_standardized.select(\"Car\", \"standardized_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación del Código:\n",
    "\n",
    "- **MinMaxScaler**:\n",
    "  - **VectorAssembler**: Convierte las columnas seleccionadas en un vector para que el `MinMaxScaler` pueda operar sobre ellas.\n",
    "  - **MinMaxScaler**: Normaliza los datos a un rango de [0, 1]. La columna `features` es la que contiene los datos a normalizar, y la columna `scaled_features` será el resultado.\n",
    "\n",
    "- **StandardScaler**:\n",
    "  - **StandardScaler**: Realiza una estandarización de las características, es decir, ajusta los datos de modo que tengan una media de 0 y una desviación estándar de 1.\n",
    "  - Similar al `MinMaxScaler`, se crea un ensamblador de características y luego se aplica el `StandardScaler` a los datos para obtener las características estandarizadas.\n",
    "\n",
    "### Resumen:\n",
    "- **Min-Max Normalization** asegura que los datos estén dentro de un rango [0, 1], útil cuando es importante mantener los valores en ese rango.\n",
    "- **Standardization (z-score)** ajusta los datos para que tengan una media de 0 y una desviación estándar de 1, y es útil cuando las características tienen unidades diferentes o distribuciones sesgadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETL-with-Python-and-PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
